{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unzip\n",
        "!pip install -r requirements.txt\n",
        "!pip install utils"
      ],
      "metadata": {
        "id": "LycU1cH1GFvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e220511e-290f-4305-ec9a-ba888b4f46ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unzip\n",
            "  Downloading unzip-1.0.0.tar.gz (704 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unzip\n",
            "  Building wheel for unzip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1281 sha256=520350f15ae9d5cfc16ec57cd103221c829c2d2214c100327e9b76477269f9d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/4d/b3/ddd83a91322fba02a91898d3b006090d1df1d3b0ad61bd8b36\n",
            "Successfully built unzip\n",
            "Installing collected packages: unzip\n",
            "Successfully installed unzip-1.0.0\n",
            "Collecting gensim (from -r requirements.txt (line 1))\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (6.17.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (7.34.0)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (5.3.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (3.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.9.1)\n",
            "Collecting num2words (from -r requirements.txt (line 9))\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.0.2)\n",
            "Collecting pandas==1.5.3 (from -r requirements.txt (line 11))\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting plotly_express (from -r requirements.txt (line 12))\n",
            "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pyDAWG (from -r requirements.txt (line 13))\n",
            "  Downloading pyDAWG-1.0.1.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyLDAvis (from -r requirements.txt (line 14))\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (1.14.1)\n",
            "Collecting sklearn_crfsuite (from -r requirements.txt (line 17))\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting stop_words (from -r requirements.txt (line 18))\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (2.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (4.67.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (1.9.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r requirements.txt (line 11)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r requirements.txt (line 11)) (2025.2)\n",
            "Collecting numpy (from -r requirements.txt (line 10))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from -r requirements.txt (line 16))\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->-r requirements.txt (line 1)) (7.1.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->-r requirements.txt (line 2)) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->-r requirements.txt (line 2)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->-r requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->-r requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->-r requirements.txt (line 2)) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->-r requirements.txt (line 2)) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->-r requirements.txt (line 2)) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 3)) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 3))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 3)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 3)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 3)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 3)) (4.9.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->-r requirements.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->-r requirements.txt (line 6)) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->-r requirements.txt (line 6)) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->-r requirements.txt (line 6)) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->-r requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 7)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 7)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 8)) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 8)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 8)) (2024.11.6)\n",
            "Collecting docopt>=0.6.2 (from num2words->-r requirements.txt (line 9))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from plotly_express->-r requirements.txt (line 12)) (5.24.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from plotly_express->-r requirements.txt (line 12)) (0.14.4)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.11/dist-packages (from plotly_express->-r requirements.txt (line 12)) (1.0.1)\n",
            "INFO: pip is looking at multiple versions of pyldavis to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyLDAvis (from -r requirements.txt (line 14))\n",
            "  Downloading pyLDAvis-3.4.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis->-r requirements.txt (line 14)) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis->-r requirements.txt (line 14)) (2.10.2)\n",
            "Collecting funcy (from pyLDAvis->-r requirements.txt (line 14))\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (3.6.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite->-r requirements.txt (line 17))\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite->-r requirements.txt (line 17)) (0.9.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.32.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 19)) (0.45.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 2)) (5.7.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.1.0->plotly_express->-r requirements.txt (line 12)) (9.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 3)) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 19)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 19)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 19)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 19)) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 19)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 19)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 19)) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis->-r requirements.txt (line 14)) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 2)) (4.3.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->-r requirements.txt (line 6)) (0.1.2)\n",
            "Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
            "Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Building wheels for collected packages: pyDAWG, stop_words, docopt\n",
            "  Building wheel for pyDAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDAWG: filename=pyDAWG-1.0.1-cp311-cp311-linux_x86_64.whl size=62785 sha256=f4556e9d0b32b9d35c893a9e35e27dbbce5da169816cde90e4a944b281b5e9d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/42/a5/2904aff781887b41600fdaf29aacb283e484d338c6932cdd25\n",
            "  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32894 sha256=9064722b7df7a125157dde0b849a4276eae593828354f6af56b7763611e5be16\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/a5/51/a5405e1da5d178491b79d12cc81b6cb9bb14fe2c8c632eba70\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=2b3e2b3ff2ace082f1c267a193f9887d9184da5374ba5505eee7cdf44f526581\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built pyDAWG stop_words docopt\n",
            "Installing collected packages: stop_words, pyDAWG, funcy, docopt, python-crfsuite, numpy, num2words, jedi, scipy, pandas, gensim, sklearn_crfsuite, pyLDAvis, plotly_express\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "mizani 0.13.2 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed docopt-0.6.2 funcy-2.0 gensim-4.3.3 jedi-0.19.2 num2words-0.5.14 numpy-1.26.4 pandas-1.5.3 plotly_express-0.4.1 pyDAWG-1.0.1 pyLDAvis-3.4.0 python-crfsuite-0.9.11 scipy-1.13.1 sklearn_crfsuite-0.5.0 stop_words-2018.7.23\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=ab3585f48c81a15fa1999d9914ae720273cc6caab51869e6f284a2fd8e3affe2\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/0c/b3/674aea8c5d91c642c817d4d630bd58faa316724b136844094d\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kix6HEEayKD"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "El objetivo principal de los algoritmos de _matching_ es el de, dado un fragmento de texto, encontrar, de entre un conjunto de candidatos, los textos más similares al fragmento orginial.\n",
        "\n",
        "Como texto podemos pensar tanto en palabras, en pequeñas frases o en documentos enteros.\n",
        "\n",
        "Podemos pensar en 3 tipos de técnicas de matching:\n",
        "- **Coincidencia exacta**: ya vimos ejemplos de este tipo al estudiar la **Distancia de Edición**.\n",
        "    - A nivel de carácter: strings que difieren en caracteres\n",
        "    - A nivel de token: strings que difieren en palabras\n",
        "    - Fonéticos: palabras que suenan de manera similar\n",
        "- **Coincidencia aproximada o difusa**\n",
        "- **Coincidencia mediante aproximaciones**\n",
        "\n",
        "| Candidato / Tipo de resultado \t| Exacta \t| Aproximada \t| Transformación \t|\n",
        "|-\t|-\t|-\t|-\t|\n",
        "| String \t| Comparación de strings \t| Comparación difusa \t| Ontologías \t|\n",
        "| Categoría \t| Gramáticas \t| Reconocimiento probabilístico \t| Análisis de topics \t|\n",
        "| Documento \t| - \t| Recuperación de información \t| Traducción automática \t|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fAtoLRqcxV-"
      },
      "source": [
        "# Regular expressions (Regex)\n",
        "\n",
        "Muy utilizadas (y conocidas) suelen emplearse al limpiar el texto o buscar formatos dentro del texto. A modo introductorio, las expresiones regulares son una forma de finite state automaton.\n",
        "\n",
        "<img src=http://www.cs.cornell.edu/courses/cs312/2006fa/recitations/images/dfa-examples.gif>\n",
        "\n",
        "Son grafos que siguen una secuencia que nosotros definimos. Por ejemplo, el grafo de la izquierda, solo podría generar expresiones como ab, abb, abbb, abbbb y así hasta el infinito. El de la derecha, podría generar expresiones como abcb, o abbb, abbbbbb, por ejemplo.\n",
        "\n",
        "Conceptualmente, las regex _funcionan_ así _por debajo_. Lógicamente cuando las usamos es mucho más fácil, ¿verdad :D?\n",
        "\n",
        "La definición de estos grafos es posible mediante la [librería de Python re](https://docs.python.org/3/library/re.html), módulo del paquete base de Python dedicado a las expresiones regulares.\n",
        "\n",
        "Cierto es que no siempre nos hará falta. Algunas veces con un simple _string.replace()_ o _string.find()_ tendremos suficiente. No obstante, para muchas tareas son bastante útiles.\n",
        "\n",
        "Algunas tareas típicas en las que se utilizan son la búsqueda (y a veces normalización) de emails, urls, numeros de telefono, etc. Solo la extracción es interesante, pero mediante su normalización nos permite reducir la cardinalidad del vocabulario y asociar entidades similares a un mismo alias.\n",
        "\n",
        "[Regex Online](https://regexr.com/) es uno de los mejores recursos online para visualizar que hacen los regex\n",
        "\n",
        "Veamos algunos ejemplos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_KqmU4ZeqwtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1acJWFL4qwvr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aVNaYYv3cxV7"
      },
      "outputs": [],
      "source": [
        "# Función que nos ayudará a visualizar algunos resultados\n",
        "\n",
        "from termcolor import colored\n",
        "def test_pass(ok, text):\n",
        "    color = 'green' if ok else 'red'\n",
        "    return colored(text, color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4cn_YtqeayKI"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "maJ7nCuRayKI"
      },
      "outputs": [],
      "source": [
        "text = 'Todos los animales son iguales, pero algunos son más iguales que otros'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pH9PfXeGayKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045671a8-7629-4846-9c03-df71e552e22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "RE_TEST = re.compile(r'todos')\n",
        "print(RE_TEST.match(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": true,
        "id": "BAzhGWx2ayKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13ff198-491b-43ba-ca61-1b489b6d0bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 5), match='Todos'>\n"
          ]
        }
      ],
      "source": [
        "RE_TEST = re.compile(r'Todos')\n",
        "print(RE_TEST.match(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xGCJgs2LayKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a430e9-0ebe-4d40-debd-f12e6e72fc3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 1), match='T'>\n"
          ]
        }
      ],
      "source": [
        "RE_TEST = re.compile(r'[a-zA-Z]')\n",
        "print(RE_TEST.match(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b4AHDZcfayKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbbf4290-dcb7-4a09-fdbe-81d0c28ab51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 5), match='Todos'>\n"
          ]
        }
      ],
      "source": [
        "RE_TEST = re.compile(r'\\bTodos\\b')\n",
        "print(RE_TEST.match(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mEJooVfIayKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e0f79d-0833-4219-881e-0a0b24d51267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "RE_TEST = re.compile(r'\\bTod\\b')\n",
        "print(RE_TEST.match(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYl1HWnqcxWe"
      },
      "source": [
        "## Obtener un correo electrónico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Gdbhnbp1cxWe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "^ -> start of string\n",
        "+ -> match 1 or more preceding regex\n",
        "[^@]+\n",
        "@[^@]+\n",
        "\\. -> '.'\n",
        "\"\"\"\n",
        "\n",
        "RE_EMAIL = re.compile('[^@]+@[^@]+\\.[^@]+')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emails_list = [\n",
        "    '@invalid@adress.com',\n",
        "    'correo_valido@gmail.com',\n",
        "    'notan@valido@gmail.com',\n",
        "    'si.valido.david@gmail.com',\n",
        "    'paginaweb.com',\n",
        "    'paginaweb.com@paginaweb.com'\n",
        "]\n",
        "for email in emails_list:\n",
        "    if RE_EMAIL.match(email):\n",
        "        print(True)\n",
        "        print(test_pass(True, email))\n",
        "        print('___')\n",
        "    else:\n",
        "        print(False)\n",
        "        print(test_pass(False, email))\n",
        "        print('___')"
      ],
      "metadata": {
        "id": "rax_iSyQZfeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca801e4-5e7f-4670-d49d-7b26c8de36cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "@invalid@adress.com\n",
            "___\n",
            "True\n",
            "correo_valido@gmail.com\n",
            "___\n",
            "False\n",
            "notan@valido@gmail.com\n",
            "___\n",
            "True\n",
            "si.valido.david@gmail.com\n",
            "___\n",
            "False\n",
            "paginaweb.com\n",
            "___\n",
            "True\n",
            "paginaweb.com@paginaweb.com\n",
            "___\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guXnMjYrcxWs"
      },
      "source": [
        "## Obtener precios"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "import unicodedata\n",
        "\n",
        "CURRENCIES = ''.join(chr(i) for i in range(0xffff) if unicodedata.category(chr(i)) == 'Sc')\n",
        "RE_MONEY_GENERAL= re.compile('((\\s|^)([\\d]*)(\\.)?([\\d])*([%s]|e|USD|USD\\$|U\\$D)(\\s|$))'\n",
        "                          '|((\\s|^)([%s]|e|USD|USD\\$|U\\$D)([\\d])*(\\.)?([\\d])*(\\s|$))'%(CURRENCIES, CURRENCIES), re.IGNORECASE)\n",
        "RE_MONEY_EU= re.compile('((\\s|^)([\\d]{0,3}([\\.][\\d]{3})(,[\\d]*))([%s]|e|(USD|USD\\$|U\\$D))(\\s|$))'\n",
        "                     '|((\\s|^)([%s]|e|(USD|USD\\$|U\\$D))([\\d]{0,3}([\\.][\\d]{3})(,[\\d]*))(\\s|$))'%(CURRENCIES, CURRENCIES), re.IGNORECASE)\n",
        "RE_MONEY_EU_INVERSE= re.compile('((\\s|^)([\\d]{0,3}([,][\\d]{3})(\\.[\\d]*))([%s]|e|(USD|USD\\$|U\\$D))(\\s|$))'\n",
        "                             '|((\\s|^)([%s]|e|(USD|USD\\$|U\\$D))([\\d]{0,3}([,][\\d]{3})(\\.[\\d]*))(\\s|$))'%(CURRENCIES, CURRENCIES), re.IGNORECASE)\n"
      ],
      "metadata": {
        "id": "k3OdYu4-ZkuY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_currencies = [\n",
        "    '$20.2',\n",
        "    '$.2',\n",
        "    '$0.2',\n",
        "    '$3433.2',\n",
        "    '.2$',\n",
        "    '2.0$',\n",
        "    '2.$',\n",
        "    '2.0€',\n",
        "    '2¥',\n",
        "    '20USD',\n",
        "    '20e',\n",
        "    '20 €',\n",
        "    '20 usd',\n",
        "    '€200.123,2',\n",
        "    '2.134,56$',\n",
        "    '23232₽',\n",
        "    '334,222.20€',\n",
        "    '20U$D',\n",
        "    '$200']\n",
        "\n",
        "incorrect_currencies = [\n",
        "    'asdfsd',\n",
        "    '$asdasd',\n",
        "    '23333,444.20€',\n",
        "    '€34523sdfas',\n",
        "    '€213.sd',\n",
        "    '$3vg554.25',\n",
        "    'expensive',\n",
        "    'cheap',\n",
        "    '2342,222.90€'\n",
        "]\n",
        "\n",
        "all_currencies = correct_currencies + incorrect_currencies\n",
        "shuffle(all_currencies)\n",
        "\n",
        "for currency in all_currencies:\n",
        "    if RE_MONEY_GENERAL.match(currency) or RE_MONEY_EU.match(currency) or RE_MONEY_EU_INVERSE.match(currency):\n",
        "        print(test_pass(True, currency))\n",
        "    else:\n",
        "        print(test_pass(False, currency))"
      ],
      "metadata": {
        "id": "t4tro_maZprq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeccb6e9-a358-4b50-c60d-6cc19807b20c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "334,222.20€\n",
            "2¥\n",
            "asdfsd\n",
            ".2$\n",
            "€34523sdfas\n",
            "23232₽\n",
            "20 €\n",
            "expensive\n",
            "20 usd\n",
            "$20.2\n",
            "2342,222.90€\n",
            "2.0€\n",
            "$0.2\n",
            "20e\n",
            "$200\n",
            "$3433.2\n",
            "2.134,56$\n",
            "20U$D\n",
            "$asdasd\n",
            "2.0$\n",
            "$3vg554.25\n",
            "23333,444.20€\n",
            "€200.123,2\n",
            "$.2\n",
            "€213.sd\n",
            "2.$\n",
            "20USD\n",
            "cheap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ql50I8sayKM"
      },
      "source": [
        "# DAWG\n",
        "\n",
        "Lo presentábamos antes de manera muy  rápida, un _Directed Acyclic Word Graph_ (por sus siglas, DAWG), también llamado, _Deterministic Acyclic Finite State Automaton_ (DAFSA), es un tipo de estructura de datos que permite representar datos de tipo texto y realizar consultas.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "En el grafo generado se distinguen:\n",
        "- **Nodos**: un carácter / símbolo\n",
        "- **Vértices**: enlace con el siguiente carácter / símbolo más probable\n",
        "\n",
        "http://www.wutka.com/dawg.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFEdr2WPayKM"
      },
      "source": [
        "## Ejemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Qhm2js1tayKM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from utils import load_movie_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0TuIc8pYayKM"
      },
      "outputs": [],
      "source": [
        "datasets_path = './'\n",
        "movie_titles_file = 'films.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t4KNu8eyayKM"
      },
      "outputs": [],
      "source": [
        "movies_titles = load_movie_titles(datasets_path, movie_titles_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U1ZMHFAayKM"
      },
      "source": [
        "## Lo creamos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install shell\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u7zcVdH6ffL",
        "outputId": "cd254179-ec90-48d0-abd9-3a38fbaf7497"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shell\n",
            "  Downloading shell-1.0.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading shell-1.0.1-py2.py3-none-any.whl (5.4 kB)\n",
            "Installing collected packages: shell\n",
            "Successfully installed shell-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install DAWG"
      ],
      "metadata": {
        "id": "FxlTWnTeH2_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83961932-650d-4fca-fc50-d2eab0830eb7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting DAWG\n",
            "  Using cached DAWG-0.8.0.tar.gz (371 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: DAWG\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for DAWG\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for DAWG\n",
            "Failed to build DAWG\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (DAWG)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydawg import DAWG\n",
        "\n",
        "dawg = DAWG()\n",
        "\n",
        "for w in sorted(m.title for m in movies_titles):\n",
        "    dawg.add_word_unchecked(w)"
      ],
      "metadata": {
        "id": "2HdDedQ2bUxP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "t = random.choice(movies_titles).title\n",
        "t"
      ],
      "metadata": {
        "id": "VvRkcWaBbVAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd80ee1c-21a2-4199-a9f2-0f92afcff0e2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Getting of Wisdom'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t in dawg"
      ],
      "metadata": {
        "id": "NFVmz00kbepx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34bf9194-0706-443a-9027-ed608308f48c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9yJBxMpayKN"
      },
      "source": [
        "## Operaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_uiw_nDayKN"
      },
      "source": [
        "### Búsqueda por prefijo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in dawg.find_all('Batman'):\n",
        "    print(m)"
      ],
      "metadata": {
        "id": "vwOPcy9pbhlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef12efb-1f8c-444a-9d49-95ecc28ed621"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batman\n",
            "Batman: The Movie\n",
            "Batman Returns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFRXPGr8ayKN"
      },
      "source": [
        "### Prefijo más largo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'La guerra de nunca jamás'\n",
        "pfx = dawg.longest_prefix(s)\n",
        "print( s[:pfx])"
      ],
      "metadata": {
        "id": "lwQLERK5blzc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3394a8ef-d435-4d0f-8e7c-c70464feb622"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaf-X-WmayKN"
      },
      "source": [
        "### Búsqueda en una oración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8kiwmLNOayKN"
      },
      "outputs": [],
      "source": [
        "def token_match(dawg, tknlist):\n",
        "    for n in range(len(tknlist), 0, -1):\n",
        "        test_str = ' '.join(tknlist[:n])\n",
        "        if test_str in dawg:\n",
        "            return test_str\n",
        "\n",
        "def token_match_all(dawg, utterance):\n",
        "    tknlist = utterance.split()\n",
        "    return [token_match(dawg, tknlist[chunk:])\n",
        "             for chunk in range(len(tknlist))]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_match_all(dawg, 'Donde echan Batman y Robin esta noche')"
      ],
      "metadata": {
        "id": "dQwxITdWbpZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eddd708-4a63-4d94-fa61-5d57e21aa773"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, 'Batman', None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJOW4iy6ayKN"
      },
      "source": [
        "### Minimal perfect hash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dawg.word2index('Batman')"
      ],
      "metadata": {
        "id": "xOOG3HeZbsfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80aab0dd-c7a9-454b-8bfa-36f165f18b58"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "242"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuQDZqMbayKN"
      },
      "source": [
        "# Distancia entre textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOY33cUzayKO"
      },
      "source": [
        "[Jellyfish](https://jellyfish.readthedocs.io/en/latest/) es una librería que contiene funciones para el cálculo de similitud entre textos. Dicha similitud puede ser á nivel léxico-gráfico (strings) o fonético.\n",
        "\n",
        "\n",
        "Algoritmos de comparación de strings:\n",
        "\n",
        "- Levenshtein Distance\n",
        "- Damerau-Levenshtein Distance\n",
        "- Jaro Distance\n",
        "- Jaro-Winkler Distance\n",
        "- Match Rating Approach Comparison\n",
        "- Hamming Distance\n",
        "\n",
        "Algoritmos de encoding fonético:\n",
        "\n",
        "- American Soundex\n",
        "- Metaphone\n",
        "- NYSIIS (New York State Identification and Intelligence System)\n",
        "- Match Rating Codex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DzeeexprayKO"
      },
      "outputs": [],
      "source": [
        "# !pip3 install jellyfish\n",
        "import jellyfish\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pypi.org/project/jellyfish/"
      ],
      "metadata": {
        "id": "sMaIVJQz0R2m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA3gAeG0ayKO"
      },
      "source": [
        "## Levenshtein\n",
        "\n",
        "Recordemos: distancia de Edit (Edición) en la que las operaciones permitidas son la inserción, la eliminación y la sustitución."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.levenshtein_distance('Cisne negro', 'Cisne negro')"
      ],
      "metadata": {
        "id": "mYtg6_vEb3ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d85fe9-571b-4d69-e424-a68f0442586c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.levenshtein_distance('Cisne negro', 'Cisne negor')"
      ],
      "metadata": {
        "id": "CDdw0XQLb36a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd17ae5-9204-469c-aef1-976d5b79cdf3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.levenshtein_distance('Cisne negro', 'Cisne nego')"
      ],
      "metadata": {
        "id": "AXxo79Zub4Bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8b8ede-462b-43b1-e6d2-376811f8007a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.levenshtein_distance('Cisnee negro', 'Cisne nego')"
      ],
      "metadata": {
        "id": "jIHNrKG9b4Id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998646e1-6ea7-475c-d387-96247799b93e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.levenshtein_distance('Cisneee negro', 'Cisne nego')"
      ],
      "metadata": {
        "id": "t5g9Xy2bb4Zl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cad20b5-2bd1-4e14-d3bd-976c1c9549fb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWAXMBK4ayKO"
      },
      "source": [
        "## Damerau-Levenshtein\n",
        "\n",
        "Recordemos: distancia de Edit (Edición) en la que las operaciones permitidas son la inserción, la eliminación y la transposición de 2 caracteres adyacentes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.damerau_levenshtein_distance('Cisne negro', 'Cisne negro')"
      ],
      "metadata": {
        "id": "3JJLjn0wcDwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42cfb199-bb14-43da-dd1c-e036611feadb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.damerau_levenshtein_distance('Cisne negro', 'Cisne negor')"
      ],
      "metadata": {
        "id": "a1VfTf1CcD3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d8ff98-6fc1-42c4-dd64-209b5102e19c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.damerau_levenshtein_distance('Cisne negro', 'Cisne nego')"
      ],
      "metadata": {
        "id": "-dJgKMnHcD-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00601f39-6410-4f18-b39e-81d07bb62700"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.damerau_levenshtein_distance('Cisnee negro', 'Cisne nego')"
      ],
      "metadata": {
        "id": "jmW6hmMzcEF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd3d1e9-0987-4d23-eede-170af22de56d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jellyfish.damerau_levenshtein_distance('Cisneee negro', 'Cisne nego')"
      ],
      "metadata": {
        "id": "yjEsgBwkcEQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a542be-7245-4f80-bde4-4b57ba5b33eb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}